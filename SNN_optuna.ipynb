{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SNN_optuna.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "nKig4zAiqtfm"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niobeus/snn_classification/blob/main/SNN_optuna.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX7hkfDPp-aU"
      },
      "source": [
        "#Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GG_-RZs4qCtt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57ad8a3f-c76b-45a3-d467-96e2e2627037"
      },
      "source": [
        "!pip install category_encoders"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.2.2-py2.py3-none-any.whl (80 kB)\n",
            "\u001b[?25l\r\u001b[K     |████                            | 10 kB 17.7 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 20 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 30 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 40 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 51 kB 2.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 61 kB 2.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 71 kB 2.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 80 kB 2.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (0.10.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.4.1)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (0.5.1)\n",
            "Requirement already satisfied: pandas>=0.21.1 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.1->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.1->category_encoders) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.5.1->category_encoders) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.0.1)\n",
            "Installing collected packages: category-encoders\n",
            "Successfully installed category-encoders-2.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEmhc8zJqGuq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b691b29-2685-491e-b9bb-0248ce2d812c"
      },
      "source": [
        "from category_encoders import LeaveOneOutEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import os\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from torch.utils.data import Dataset"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDZTYAfOqINn"
      },
      "source": [
        "def load_dataset(URL, target_name, cat_features, test_size=0.2, val_size=0.1):\n",
        "  df = pd.read_csv(URL)\n",
        "  target = df.pop(target_name)\n",
        "\n",
        "  X, X_test, y, y_test = train_test_split(df, target, test_size=test_size)\n",
        "  \n",
        "  class_to_int = {c: i for i, c in enumerate(y.unique())}                                                                                                               \n",
        "  y_int = [class_to_int[v] for v in y]                                                                                                                            \n",
        "  y_test_int = [class_to_int[v] for v in y_test] \n",
        "\n",
        "  # encode categorical features\n",
        "  cat_encoder = LeaveOneOutEncoder()\n",
        "  cat_encoder.fit(X, y_int)\n",
        "  X = cat_encoder.transform(X)\n",
        "\n",
        "  X_test = cat_encoder.transform(X_test)\n",
        "\n",
        "  X = X.values.astype('float32')\n",
        "  X_test = X_test.values.astype('float32')\n",
        "  y = np.array(y_int)\n",
        "  y_test = np.array(y_test_int)\n",
        "\n",
        "  if val_size:\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=val_size)\n",
        "    return X_train, X_test, X_val, y_train, y_test, y_val\n",
        "\n",
        "  return X, X_test, y, y_test"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ01wrp1weoO"
      },
      "source": [
        "# datasets information\n",
        "datasets = {\n",
        "    'adult' : {\n",
        "        'URL' : 'https://docs.google.com/uc?id=10eFO2rVlsQBUffn0b7UCAp28n0mkLCy7&export=download',\n",
        "        'target_name' : '<=50K',\n",
        "        'cat_features' : ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
        "        },\n",
        "\n",
        "    'airlines' : {\n",
        "        'URL' : 'https://www.openml.org/data/get_csv/66526/phpvcoG8S',\n",
        "        'target_name' : 'Delay',\n",
        "        'cat_features' : ['Airline', 'Flight', 'AirportFrom', 'AirportTo', 'DayOfWeek']\n",
        "    },\n",
        "\n",
        "    'albert' : {\n",
        "        'URL' : 'https://www.openml.org/data/get_csv/19335520/file7b53746cbda2.arff',\n",
        "        'target_name' : 'class',\n",
        "        'cat_features' : []\n",
        "        },\n",
        "\n",
        "    'bank' : {\n",
        "        'URL' : 'https://www.openml.org/data/get_csv/1586218/phpkIxskf',\n",
        "        'target_name' : 'Class',\n",
        "        'cat_features' : []\n",
        "    },\n",
        "\n",
        "    'blastchar' : {\n",
        "        'URL' : 'https://vk.com/doc166590718_613866185',\n",
        "        'target_name' : 'Churn',\n",
        "        'cat_features' : ['gender', 'Partner', 'Dependents', 'PhoneService', \n",
        "                          'MultipleLines', 'InternetService', 'OnlineSecurity', \n",
        "                          'OnlineBackup', 'DeviceProtection', 'TechSupport', \n",
        "                          'StreamingTV', 'StreamingMovies', 'Contract', \n",
        "                          'PaperlessBilling', 'PaymentMethod']\n",
        "    },\n",
        "\n",
        "    'jasmine' : {\n",
        "        'URL' : 'https://www.openml.org/data/get_csv/19335516/file79b563a1a18.arff',\n",
        "        'target_name' : 'class',\n",
        "        'cat_features' : []\n",
        "    },\n",
        "\n",
        "    # this dataset is not in public access\n",
        "    # 'philippine' : {\n",
        "        # 'URL' : 'http://www.causality.inf.ethz.ch/AutoML/philippine.zip',\n",
        "        # 'target_name' : 'Delay',\n",
        "        # 'cat_features' : ['Airline', 'Flight', 'AirportFrom', 'AirportTo', 'DayOfWeek']\n",
        "    # },\n",
        "\n",
        "    'shrutime' : {\n",
        "        'URL' : 'https://vk.com/doc166590718_613869835',\n",
        "        'target_name' : 'Exited',\n",
        "        'cat_features' : ['Surname', 'Geography', 'Gender']\n",
        "    },\n",
        "\n",
        "    'spambase' : {\n",
        "        'URL' : 'https://vk.com/doc166590718_613870389',\n",
        "        'target_name' : 'class',\n",
        "        'cat_features' : []\n",
        "    },\n",
        "\n",
        "    'QSARbio' : {\n",
        "        'URL' : 'https://vk.com/doc166590718_613865746',\n",
        "        'target_name' : 'class',\n",
        "        'cat_features' : []\n",
        "    }\n",
        "}"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKig4zAiqtfm"
      },
      "source": [
        "#SNN Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSF2lkKe7C1n"
      },
      "source": [
        "# handlers for dataloader\n",
        "class DataSet(Dataset):\n",
        "  def __init__(self, x, y):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x)\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "    return torch.tensor(self.x[i]), torch.tensor(self.y[i])\n",
        "\n",
        "class Sampler:\n",
        "  def __init__(self, X, y):\n",
        "    self.dataset = DataSet(X, y)\n",
        "  def sample(self, batch_size):\n",
        "    n = len(self.dataset)\n",
        "    idxs = torch.randperm(n)\n",
        "    for i in range(0, n, batch_size):\n",
        "      yield self.dataset[idxs[i: i + batch_size]]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2RV0XLi-WT8"
      },
      "source": [
        "def layer(input, output):\n",
        "  return nn.Sequential(nn.Linear(input, output), nn.SELU(), nn.AlphaDropout(p=0.2))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFeN_8Hu-bNo"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "  \"\"\"\n",
        "  Main model for SNN classifier.\n",
        "\n",
        "  Parameters:\n",
        "  ----------------------\n",
        "  n_input : int\n",
        "    The input size.\n",
        "  n_hidden : list\n",
        "    This list describe hidden layers of SNN. \n",
        "    The len of list is the number of hidden layers.\n",
        "    Each element shows how many neurons are contained in the corresponding layer.\n",
        "  n_output : int\n",
        "    The output size.\n",
        "      \n",
        "  \"\"\"\n",
        "  def __init__(self, n_input, n_hidden, n_output):\n",
        "    super(MLP, self).__init__()\n",
        "    self.n_output = n_output\n",
        "    n_hidden = [n_input] + n_hidden\n",
        "    layers = [layer(n_hidden[i], n_hidden[i+1]) for i in range(len(n_hidden) - 1)]\n",
        "    layers.append(nn.Sequential(nn.Linear(n_hidden[-1], n_output)))\n",
        "    \n",
        "    self.model = nn.Sequential(*layers)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = self.model(x)\n",
        "    return x"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huffh41O4ldQ"
      },
      "source": [
        "#Оптимизация гиперпараметров с помощью Optuna"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNozZvnm5DlJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a1ef89b-45f5-4cb3-8168-b07f060ded12"
      },
      "source": [
        "! pip install optuna"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-2.9.1-py3-none-any.whl (302 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 17.0 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 30 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 51 kB 2.2 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 61 kB 2.4 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 71 kB 2.5 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 81 kB 2.8 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 92 kB 2.9 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 102 kB 2.3 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 112 kB 2.3 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 122 kB 2.3 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 133 kB 2.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 143 kB 2.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 153 kB 2.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 163 kB 2.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 174 kB 2.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 184 kB 2.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 194 kB 2.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 204 kB 2.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 215 kB 2.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 225 kB 2.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 235 kB 2.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 245 kB 2.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 256 kB 2.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 266 kB 2.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 276 kB 2.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 286 kB 2.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 296 kB 2.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 302 kB 2.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.19.5)\n",
            "Collecting cmaes>=0.8.2\n",
            "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (3.13)\n",
            "Collecting alembic\n",
            "  Downloading alembic-1.7.3-py3-none-any.whl (208 kB)\n",
            "\u001b[K     |████████████████████████████████| 208 kB 41.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.62.3)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.1)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.4.1-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.25)\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.9.0-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 9.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (2.4.7)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (4.8.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (1.1.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (5.2.2)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.1.5-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 4.1 MB/s \n",
            "\u001b[?25hCollecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.4.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (2.2.0)\n",
            "Collecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.2.0-py3-none-any.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 45.9 MB/s \n",
            "\u001b[?25hCollecting autopage>=0.4.0\n",
            "  Downloading autopage-0.4.0-py3-none-any.whl (20 kB)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.6.0-py2.py3-none-any.whl (111 kB)\n",
            "\u001b[K     |████████████████████████████████| 111 kB 42.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (3.7.4.3)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Collecting colorama>=0.3.7\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (21.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna) (2.0.1)\n",
            "Building wheels for collected packages: pyperclip\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11136 sha256=bdb1ede1e687cbb3d993aaab8d3b4c475408c808c658ac3f48a235361f276222\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "Successfully built pyperclip\n",
            "Installing collected packages: pyperclip, pbr, colorama, stevedore, Mako, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n",
            "Successfully installed Mako-1.1.5 alembic-1.7.3 autopage-0.4.0 cliff-3.9.0 cmaes-0.8.2 cmd2-2.2.0 colorama-0.4.4 colorlog-6.4.1 optuna-2.9.1 pbr-5.6.0 pyperclip-1.8.2 stevedore-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CCPGsVfbEaf"
      },
      "source": [
        "import optuna\n",
        "from optuna.trial import TrialState"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03eiylam9DyD"
      },
      "source": [
        "DEVICE = torch.device(\"cuda\")\n",
        "BATCHSIZE = 128\n",
        "DIR = os.getcwd()\n",
        "EPOCHS = 10\n",
        "LOSS = nn.CrossEntropyLoss()\n",
        "\n",
        "dataset_info = datasets['adult']\n",
        "X_train, X_test, X_valid, y_train, y_test, y_valid = load_dataset(dataset_info['URL'], dataset_info['target_name'], dataset_info['cat_features'], val_size=0.2)\n",
        "\n",
        "CLASSES = len(set(y_train))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8ihvF7Fi6XY"
      },
      "source": [
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        torch.nn.init.xavier_normal_(tensor, gain=nn.init.calculate_gain('selu'))\n",
        "        torch.nn.init.zero_(m.bias)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2Wm26xj9HeS"
      },
      "source": [
        "def define_model(trial):\n",
        "    # We optimize the number of layers, hidden units and dropout ratio in each layer.\n",
        "    n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
        "    layers = []\n",
        " \n",
        "    in_features = X_train.shape[1]\n",
        "    for i in range(n_layers):\n",
        "        out_features = trial.suggest_int(\"n_units_l{}\".format(i), 4, in_features)\n",
        "        layers.append(nn.Linear(in_features, out_features))\n",
        "        layers.append(nn.SELU())\n",
        "        p = trial.suggest_float(\"dropout_l{}\".format(i), 0.05, 0.5)\n",
        "        layers.append(nn.AlphaDropout(p))\n",
        "\n",
        "        in_features = out_features\n",
        "    layers.append(nn.Linear(in_features, CLASSES))\n",
        "    # layers.append(nn.LogSoftmax(dim=1))\n",
        "\n",
        "    return nn.Sequential(*layers)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cA8FPBzPVJH-"
      },
      "source": [
        "def print_roc_auc_score(X, y, model):\n",
        "  X = torch.tensor(X)\n",
        "  X = X.to(DEVICE)\n",
        "  print('ROC-AUC Score is: ', roc_auc_score(y_test, nn.Softmax(dim=1)(model(X)).argmax(1).to('cpu')))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvocgoAt4oTQ"
      },
      "source": [
        "def objective(trial):\n",
        "\n",
        "    # Generate the model.\n",
        "    model = define_model(trial).to(DEVICE)\n",
        "    # And init it\n",
        "    model.apply(weights_init)\n",
        "\n",
        "    # Generate the optimizers\n",
        "    # We optimize the learning rate and optimizers\n",
        "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
        "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
        "\n",
        "    # Get the dataset\n",
        "    train_sampler = Sampler(X_train, y_train)\n",
        "    valid_sampler = Sampler(X_valid, y_valid)\n",
        "\n",
        "    # Training of the model.\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        for data,target in train_sampler.sample(BATCHSIZE):\n",
        "\n",
        "            data, target = data.view(data.size(0), -1).to(DEVICE), target.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = LOSS(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation of the model.\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        with torch.no_grad():\n",
        "            \n",
        "            # Uncomment this, if you want to minimize loss function\n",
        "            # \n",
        "            # for data,target in valid_sampler.sample(BATCHSIZE):\n",
        "            #     data, target = data.view(data.size(0), -1).to(DEVICE), target.to(DEVICE)\n",
        "            #     output = model(data)\n",
        "            #     loss = LOSS(output, target)\n",
        "            # \n",
        "\n",
        "            # Uncomment this, if you want to maximize auc roc \n",
        "            # \n",
        "            out = model(torch.tensor(X_valid).to(DEVICE))\n",
        "            y_pred = nn.Softmax(dim=1)(out).argmax(1).to('cpu')\n",
        "            roc_auc = roc_auc_score(y_valid, y_pred)\n",
        "            # \n",
        "\n",
        "\n",
        "        trial.report(roc_auc, epoch)\n",
        "        # Handle pruning based on the intermediate value.\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return roc_auc"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qc4BP4oY9WZh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66ca2a0f-45b1-4fc8-f24a-22c99297258d"
      },
      "source": [
        "study = optuna.create_study(\n",
        "    # case for auc roc\n",
        "    direction=\"maximize\",\n",
        "    # Successive Halving is a bandit-based algorithm to identify the best one among multiple configurations\n",
        "    pruner=optuna.pruners.SuccessiveHalvingPruner(min_early_stopping_rate=3),\n",
        "    # Sampler using TPE (Tree-structured Parzen Estimator) algorithm.\n",
        "    sampler=optuna.samplers.TPESampler(seed=42)\n",
        ")\n",
        "\n",
        "# We start the optimization\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
        "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
        "\n",
        "print(\"Study statistics: \")\n",
        "print(\"  Number of finished trials: \", len(study.trials))\n",
        "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
        "print(\"  Number of complete trials: \", len(complete_trials))\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(\"  Value: \", trial.value)\n",
        "\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "  print(\"    {}: {}\".format(key, value))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2021-10-01 16:23:45,965]\u001b[0m A new study created in memory with name: no-name-4d2ddf02-e4c8-41c2-9914-5d3e2da7d4ed\u001b[0m\n",
            "\u001b[32m[I 2021-10-01 16:23:49,598]\u001b[0m Trial 0 finished with value: 0.5 and parameters: {'n_layers': 2, 'n_units_l0': 14, 'dropout_l0': 0.3793972738151323, 'n_units_l1': 10, 'dropout_l1': 0.12020838819909643, 'optimizer': 'SGD', 'lr': 0.002537815508265664}. Best is trial 0 with value: 0.5.\u001b[0m\n",
            "\u001b[32m[I 2021-10-01 16:23:54,234]\u001b[0m Trial 1 finished with value: 0.5032076984763432 and parameters: {'n_layers': 4, 'n_units_l0': 4, 'dropout_l0': 0.48645943347289744, 'n_units_l1': 4, 'dropout_l1': 0.4245991883601898, 'n_units_l2': 4, 'dropout_l2': 0.14555259980522428, 'n_units_l3': 4, 'dropout_l3': 0.13182123524319528, 'optimizer': 'SGD', 'lr': 0.0005342937261279777}. Best is trial 1 with value: 0.5032076984763432.\u001b[0m\n",
            "\u001b[32m[I 2021-10-01 16:23:58,226]\u001b[0m Trial 2 finished with value: 0.5008019246190858 and parameters: {'n_layers': 2, 'n_units_l0': 10, 'dropout_l0': 0.11277223729341883, 'n_units_l1': 6, 'dropout_l1': 0.2148628294821613, 'optimizer': 'RMSprop', 'lr': 0.0011400863701127321}. Best is trial 1 with value: 0.5032076984763432.\u001b[0m\n",
            "\u001b[32m[I 2021-10-01 16:24:03,096]\u001b[0m Trial 3 finished with value: 0.5020048115477145 and parameters: {'n_layers': 3, 'n_units_l0': 4, 'dropout_l0': 0.32339518335564726, 'n_units_l1': 4, 'dropout_l1': 0.1267358556592812, 'n_units_l2': 4, 'dropout_l2': 0.0792732168433758, 'optimizer': 'RMSprop', 'lr': 0.00016536937182824412}. Best is trial 1 with value: 0.5032076984763432.\u001b[0m\n",
            "\u001b[32m[I 2021-10-01 16:24:06,179]\u001b[0m Trial 4 finished with value: 0.5 and parameters: {'n_layers': 1, 'n_units_l0': 11, 'dropout_l0': 0.2480686221828206, 'optimizer': 'RMSprop', 'lr': 0.0433792069749094}. Best is trial 1 with value: 0.5032076984763432.\u001b[0m\n",
            "\u001b[32m[I 2021-10-01 16:24:10,194]\u001b[0m Trial 5 finished with value: 0.5 and parameters: {'n_layers': 2, 'n_units_l0': 11, 'dropout_l0': 0.19026998424023495, 'n_units_l1': 8, 'dropout_l1': 0.2960196257044759, 'optimizer': 'RMSprop', 'lr': 0.057279044707996205}. Best is trial 1 with value: 0.5032076984763432.\u001b[0m\n",
            "\u001b[32m[I 2021-10-01 16:24:16,758]\u001b[0m Trial 6 finished with value: 0.5 and parameters: {'n_layers': 5, 'n_units_l0': 10, 'dropout_l0': 0.46484340576040256, 'n_units_l1': 4, 'dropout_l1': 0.13819228808861533, 'n_units_l2': 4, 'dropout_l2': 0.07035228000974214, 'n_units_l3': 4, 'dropout_l3': 0.19639864884346897, 'n_units_l4': 4, 'dropout_l4': 0.2249047803602669, 'optimizer': 'RMSprop', 'lr': 0.000132965214572995}. Best is trial 1 with value: 0.5032076984763432.\u001b[0m\n",
            "\u001b[32m[I 2021-10-01 16:24:20,795]\u001b[0m Trial 7 finished with value: 0.5 and parameters: {'n_layers': 3, 'n_units_l0': 5, 'dropout_l0': 0.41098864133931784, 'n_units_l1': 4, 'dropout_l1': 0.49409912147023277, 'n_units_l2': 4, 'dropout_l2': 0.39751014618349584, 'optimizer': 'SGD', 'lr': 0.00672093005015611}. Best is trial 1 with value: 0.5032076984763432.\u001b[0m\n",
            "\u001b[32m[I 2021-10-01 16:24:25,455]\u001b[0m Trial 8 finished with value: 0.5 and parameters: {'n_layers': 4, 'n_units_l0': 12, 'dropout_l0': 0.08332009328034067, 'n_units_l1': 7, 'dropout_l1': 0.10214107678630838, 'n_units_l2': 7, 'dropout_l2': 0.3304841570724011, 'n_units_l3': 5, 'dropout_l3': 0.07860125762871065, 'optimizer': 'SGD', 'lr': 0.003550012525851159}. Best is trial 1 with value: 0.5032076984763432.\u001b[0m\n",
            "\u001b[32m[I 2021-10-01 16:24:30,741]\u001b[0m Trial 9 finished with value: 0.5216744258893562 and parameters: {'n_layers': 5, 'n_units_l0': 9, 'dropout_l0': 0.10381741067223577, 'n_units_l1': 8, 'dropout_l1': 0.3923532718776038, 'n_units_l2': 6, 'dropout_l2': 0.39693523097955247, 'n_units_l3': 5, 'dropout_l3': 0.28522977322189735, 'n_units_l4': 4, 'dropout_l4': 0.061438607034842836, 'optimizer': 'SGD', 'lr': 0.00018089390092767128}. Best is trial 9 with value: 0.5216744258893562.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Study statistics: \n",
            "  Number of finished trials:  10\n",
            "  Number of pruned trials:  0\n",
            "  Number of complete trials:  10\n",
            "Best trial:\n",
            "  Value:  0.5216744258893562\n",
            "  Params: \n",
            "    n_layers: 5\n",
            "    n_units_l0: 9\n",
            "    dropout_l0: 0.10381741067223577\n",
            "    n_units_l1: 8\n",
            "    dropout_l1: 0.3923532718776038\n",
            "    n_units_l2: 6\n",
            "    dropout_l2: 0.39693523097955247\n",
            "    n_units_l3: 5\n",
            "    dropout_l3: 0.28522977322189735\n",
            "    n_units_l4: 4\n",
            "    dropout_l4: 0.061438607034842836\n",
            "    optimizer: SGD\n",
            "    lr: 0.00018089390092767128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hczNfYOdk1O"
      },
      "source": [
        "#Тестирование модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZY8nzF4S-dPB"
      },
      "source": [
        "class NNClassifier:\n",
        "  \"\"\"\n",
        "  The SNN model with scikit-learn interface.\n",
        "\n",
        "  Parameters:\n",
        "  -----------------------------\n",
        "  is_swats : bool\n",
        "  SWATS allows you to change optimizer in learning process\n",
        "\n",
        "  swats_n_epochs : int\n",
        "  At what epoch SWATS should be applied\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self, MLP, batch_size, max_epochs, loss, optimizer, is_swats=True, swats_n_epochs=10, lr=0.1, device='cuda'):\n",
        "    self.MLP = MLP.to(device)\n",
        "    self.sampler = Sampler\n",
        "    self.batch_size = batch_size\n",
        "    self.optimizer = optimizer(self.MLP.parameters(), lr=lr)\n",
        "    self.is_swats = is_swats\n",
        "    self.swats_n_epochs = swats_n_epochs\n",
        "    self.lr = lr\n",
        "    self.loss = loss\n",
        "    self.device = device\n",
        "    self.max_epochs = max_epochs\n",
        "  \n",
        "  def fit(self, X_train, y_train):\n",
        "    self.MLP.train()\n",
        "    for i in range(self.max_epochs):\n",
        "      sum_loss = 0\n",
        "\n",
        "      if self.is_swats and i == (self.max_epochs - self.swats_n_epochs):\n",
        "        self.optimizer = torch.optim.AdamW(self.MLP.parameters())\n",
        "\n",
        "      # for g in self.optimizer.param_groups:\n",
        "      #   g['lr'] = g['lr'] / 1.5\n",
        "      \n",
        "      for X,y in self.sampler(X_train, y_train).sample(self.batch_size):\n",
        "        X, y = X.to(self.device), y.to(self.device)\n",
        "\n",
        "        pred = self.MLP.forward(X)\n",
        "        loss = self.loss(pred, y)\n",
        "        sum_loss += loss.item()\n",
        "        \n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "      \n",
        "      print('Epoch: {} Train loss: {:.5f}'.format(i, sum_loss / (len(y_train) /self.batch_size)))\n",
        "      \n",
        "  def predict(self, X):\n",
        "    self.MLP.eval()\n",
        "    with torch.no_grad():\n",
        "      return self.predict_proba(X).argmax(1)\n",
        "    \n",
        "  def predict_proba(self, X):\n",
        "    self.MLP.eval()\n",
        "    with torch.no_grad():\n",
        "      X = torch.tensor(X)\n",
        "      X = X.to(self.device)\n",
        "      return nn.Softmax(dim=1)(self.MLP.forward(X))\n",
        "\n",
        "  def score(self, X_test, y_test):\n",
        "    self.MLP.eval()\n",
        "    size = len(y_test)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for X,y in self.sampler(X_test, y_test).sample(self.batch_size):\n",
        "        X, y = X.to(self.device), y.to(self.device)\n",
        "        pred = self.MLP.forward(X)\n",
        "        test_loss += self.loss(pred, y).item()\n",
        "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= len(y_test) / self.batch_size\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMuf5M7wdwKQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe5cf2be-de51-46bb-b322-17c0a8c361dd"
      },
      "source": [
        "n_input = X_train.shape[1]\n",
        "n_output = len(set(y_train))\n",
        "n_hidden = [6, 5, 5]\n",
        "print(n_input, n_output)\n",
        "print(n_hidden)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14 2\n",
            "[6, 5, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXNVzsArAs2d"
      },
      "source": [
        "mlp = MLP(n_input=n_input, \n",
        "          n_hidden=n_hidden, \n",
        "          n_output=n_output)\n",
        "\n",
        "mlp.apply(weights_init)\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 12\n",
        "    \n",
        "net = NNClassifier(mlp, \n",
        "                   batch_size=batch_size, \n",
        "                   optimizer=torch.optim.Adam,\n",
        "                   is_swats=False,\n",
        "                   swats_n_epochs=10,\n",
        "                   lr=0.0015, \n",
        "                   loss=nn.CrossEntropyLoss(),\n",
        "                   device='cpu', \n",
        "                   max_epochs=epochs)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R2N4FiIAwQS"
      },
      "source": [
        "# %%time\n",
        "%%script false --no-raise-error\n",
        "net.fit(X_train,y_train)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6C8nQ4b0MK7v"
      },
      "source": [
        "%%script false --no-raise-error\n",
        "roc_auc_score(y_test, net.predict(X_test).to('cpu'))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZHm2GrvA13L"
      },
      "source": [
        "%%script false --no-raise-error\n",
        "net.score(X_test, y_test)"
      ],
      "execution_count": 30,
      "outputs": []
    }
  ]
}